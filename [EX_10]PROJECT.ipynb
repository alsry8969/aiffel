{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d6d02a",
   "metadata": {},
   "source": [
    "# 10-10. 프로젝트 : 단어 Level로 번역기 업그레이드하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f5691",
   "metadata": {},
   "source": [
    "프로젝트를 진행하기 전에 주요 라이브러리 버전을 확인해 보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d283b99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c2071",
   "metadata": {},
   "source": [
    "실습에서 구현한 번역기는 글자 단위(Character-level)에서 구현된 번역기였습니다. 하지만 실제 번역기의 경우에는 글자 단위가 아니라 단어 단위(Word-level)에서 구현되는 것이 좀 더 보편적입니다.\n",
    "\n",
    "동일한 데이터셋을 사용하면서 글자 단위와는 다른 전처리와 to_categorical() 함수가 아닌 임베딩 층(Embedding layer)를 추가하여 단어 단위의 번역기를 완성시켜보겠습니다. 하지만, 단어 단위로 할 경우에는 단어의 개수가 글자 단위로 했을 경우와 비교하여 단어장의 크기(Vocabulary) 크기도 커지고, 학습 속도도 좀 더 느려집니다. 학습과 테스트 시의 원활한 진행을 위해서 데이터에서 상위 33,000개의 샘플만 사용해주세요.\n",
    "\n",
    "33000개 중 3000개는 테스트 데이터로 분리하여 모델을 학습한 후에 번역을 테스트 하는 용도로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e392e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3888c28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 217975\n"
     ]
    }
   ],
   "source": [
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cdb215d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eng         fra\n",
       "0  Go.        Va !\n",
       "1  Go.     Marche.\n",
       "2  Go.  En route !\n",
       "3  Go.     Bouge !\n",
       "4  Hi.     Salut !"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:33000] # 33000개 샘플 사용\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1318795f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Go.', 'Go.', 'Go.', ..., 'I want to see that.',\n",
       "       'I want to see them.', 'I want to see this.'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_np_eng= lines['eng'].to_numpy()\n",
    "lines_np_fra= lines['fra'].to_numpy()\n",
    "lines_np_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86394eb2",
   "metadata": {},
   "source": [
    "## Step 1. 정제, 정규화, 전처리 (영어, 프랑스어 모두!)\n",
    "1. 구두점(Punctuation)을 단어와 분리해주세요.\n",
    "\n",
    "일반적으로 영어권 언어의 경우에는 띄어쓰기 단위로 단어를 분리합니다. 토큰화(Tokenization) 라고도 불리는 이 작업은 어디서부터 어디까지가 하나의 단어인지를 구분하는 작업인데요, 그런데 띄어쓰기를 해주기 전에 구두점을 분리하는 작업이 필요할 때가 있습니다.\n",
    "예를 들어서 'he is a good boy!'라는 문장이 있을 때, 이를 띄어쓰기 단위로 토큰화한다면 ['he', 'is', 'a', 'good', 'boy!']가 됩니다. 그런데 실제로 !는 boy와 붙어있는 한 단어가 아니므로 좀 더 올바른 전처리는 ['he', 'is', 'a', 'good', 'boy', '!']가 맞습니다.\n",
    "!나 ? 또는 온점과 같은 특수문자들을 구두점(punctuation)이라고 부릅니다. 이들을 토큰화하기 전에 단어와 미리 분리시켜주세요!\n",
    "\n",
    "분리 전 : he is a Good boy!\n",
    "분리 후 : he is a Good boy !\n",
    "\n",
    "2. 소문자로 바꿔주세요.\n",
    "\n",
    "기계가 보기에는 스펠링이 같더라도 대문자로 된 단어와 소문자로 된 단어는 서로 다른 단어입니다. 예를 들어 'Good'과 'good'은 기계가 보기에는 다른 단어입니다. 그래서 모든 문장에 대해서 전부 영어로 바꿔주는 작업을 하겠습니다.\n",
    "\n",
    "    - 변환 전 : he is a Good boy !\n",
    "    - 변환 후 : he is a good boy !\n",
    "\n",
    "3. 띄어쓰기 단위로 토큰화를 수행하세요.\n",
    "\n",
    "띄어쓰기 단위로 토큰화를 수행해서 단어를 분리하는 작업을 해주세요. 기계는 이렇게 분리된 토큰들을 각각 하나의 단어로 인식할 수 있게 됩니다.\n",
    "\n",
    "    - 토큰화 전 : 'he is a good boy !'\n",
    "    - 토큰화 후 : ['he', 'is', 'a', 'good', 'boy', '!']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a823136",
   "metadata": {},
   "source": [
    "## Step 2. 디코더의 문장에 시작 토큰과 종료 토큰을 넣어주세요.\n",
    "글자 단위 번역기를 구현할 때와 마찬가지로 디코더의 입력 시퀀스 맨 앞에는 시작을 의미하는 토큰인 <sos>가 필요합니다. 그리고 교사 강요를 수행할 때, 디코더의 실제값이 되는 디코더의 레이블 시퀀스에는 종료를 의미하는 종료 토큰 <eos>가 필요합니다.\n",
    "예를 들어 번역 문장이 \"Courez!\" 였다고 한다면, Step 1을 거친 후에는 다음과 같은 결과를 얻습니다.\n",
    "\n",
    "- Step 1을 수행한 후 : ['courez', '!']\n",
    "\n",
    "이 문장에 대해서 각각 디코더의 입력 시퀀스와 레이블 시퀀스를 만들면 다음과 같습니다.\n",
    "\n",
    "- 입력 시퀀스 : ['< sos >', 'courez', '!']\n",
    "- 레이블 시퀀스 : ['courez', '!', '< eos >']\n",
    "\n",
    "참고로 Step 2가 반드시 Step 1이 끝난 후에 이루어질 필요는 없습니다!\n",
    "Step 1을 수행하는 중간에 수행해도 상관없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd02f5",
   "metadata": {},
   "source": [
    "## Step 3. 케라스의 토크나이저로 텍스트를 숫자로 바꿔보세요.\n",
    "딥러닝 모델은 텍스트가 아닌 숫자를 처리합니다. 케라스 토크나이저를 사용해서 각 단어를 고유한 정수로 바꿔보세요.\n",
    "케라스 토크나이저의 사용법은 아래의 링크에서 2. 케라스(Keras)의 텍스트 전처리에 설명되어 있습니다.\n",
    "\n",
    "- https://wikidocs.net/31766\n",
    "\n",
    "위 링크의 가이드를 통해서 영어와 프랑스어에 대한 토크나이저를 각각 생성하고, tokenizer.texts_to_sequences()를 사용하여 모든 샘플에 대해서 정수 시퀀스로 변환해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce03b4",
   "metadata": {},
   "source": [
    "### 소문자로 바꾸고 구두점을 단어와 분리하기 + 시작 토큰과 종료 토큰 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2a8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰 및 종료 토큰\n",
    "sos_token = '<start> '\n",
    "eos_token = ' <end>'\n",
    "\n",
    "def preprocess_line(line, plus_token = True):\n",
    "    # 소문자로 변경하기\n",
    "    line = line.lower().strip()\n",
    "    # 구두점(Punctuation)을 단어와 분리하기\n",
    "    line = re.sub(r\"([?.!,¿])\", r\" \\1 \", line)\n",
    "    line = re.sub(r'[\" \"]+', \" \", line)\n",
    "    line = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", line)\n",
    "\n",
    "    line = line.strip()\n",
    "    \n",
    "    if plus_token == True:\n",
    "        line = sos_token + line + eos_token\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1fc384",
   "metadata": {},
   "source": [
    "### 띄어쓰기 단위로 토큰화 하기 + 텍스트를 숫자로 바꾸기(texts_to_sequences())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c99af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = Tokenizer(\n",
    "        num_words=7000,  \n",
    "        filters=' ',   \n",
    "        oov_token=\"<unk>\"  \n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)  \n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d719df",
   "metadata": {},
   "source": [
    "### 영어, 프랑스어 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b7346e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_lines = []\n",
    "fra_lines = []\n",
    "\n",
    "# eng_lines.append(lines.eng.apply(lambda x : preprocess_line(x,plus_token = False)))\n",
    "# fra_lines.append(lines.fra.apply(lambda x : preprocess_line(x),))\n",
    "\n",
    "for eng, fra in zip(lines.eng, lines.fra):\n",
    "    if len(eng) == 0: continue\n",
    "    if len(fra) == 0: continue   \n",
    "        \n",
    "    eng_lines.append(preprocess_line(eng, plus_token = False))\n",
    "    fra_lines.append(preprocess_line(fra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9de6c1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(eng_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6383afee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 74, 10, 3],\n",
       " [2, 342, 4, 3],\n",
       " [2, 28, 525, 10, 3],\n",
       " [2, 680, 10, 3],\n",
       " [2, 681, 10, 3],\n",
       " [2, 681, 4, 3],\n",
       " [2, 655, 10, 3],\n",
       " [2, 1630, 10, 3],\n",
       " [2, 169, 304, 1103, 304, 2568, 10, 3],\n",
       " [2, 1314, 10, 3]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tensor, eng_tokenizer = tokenize(eng_lines)\n",
    "fra_tensor, fra_tokenizer = tokenize(fra_lines)\n",
    "fra_tensor[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea92584",
   "metadata": {},
   "source": [
    "### input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1afca42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = eng_tensor\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[char for char in line if char != fra_tokenizer.word_index['<end>']] for line in fra_tensor]\n",
    "# 시작 토큰 제거\n",
    "decoder_target =[[char for char in line if char != fra_tokenizer.word_index['<start>']] for line in fra_tensor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772afc2",
   "metadata": {},
   "source": [
    "### padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e6ca433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(tensor):\n",
    "    total_data_text = list(tensor)\n",
    "    num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "    max_tokens = max(num_tokens)\n",
    "    \n",
    "    maxlen = int(max_tokens)\n",
    "    tensor = pad_sequences(tensor, padding='post', maxlen=maxlen)  \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32b25765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 8)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 16)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 16)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_tensor(encoder_input)\n",
    "decoder_input = pad_tensor(decoder_input)\n",
    "decoder_target = pad_tensor(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db747a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index)+1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index)+1\n",
    "\n",
    "max_eng_seq_len = encoder_input.shape[1]\n",
    "max_fra_seq_len = decoder_input.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d5d7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 4516\n",
      "프랑스어 단어장의 크기 : 7265\n",
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 16\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccdf741",
   "metadata": {},
   "source": [
    "### train, test dataset 나누기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d666e9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf6c80b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 8)\n",
      "(30000, 16)\n",
      "(30000, 16)\n",
      "(3000, 8)\n",
      "(3000, 16)\n",
      "(3000, 16)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print(encoder_input_train.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_target_train.shape)\n",
    "print(encoder_input_test.shape)\n",
    "print(decoder_input_test.shape)\n",
    "print(decoder_target_test.shape)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "7490ac50",
   "metadata": {},
   "source": [
    "## Step 4. 임베딩 층(Embedding layer) 사용하기\n",
    "이번에는 입력이 되는 각 단어를 임베딩 층을 사용하여 벡터화하겠습니다.\n",
    "임베딩 층을 사용하는 방법과 그 설명에 대해서는 아래의 링크의 1. 케라스 임베딩 층(Keras Embedding layer) 을 참고하세요.\n",
    "\n",
    "- https://wikidocs.net/33793\n",
    "\n",
    "실제 번역기 구현을 위해서 사용할 수 있는 인코더 코드의 예시는 다음과 같습니다. 이를 통해서 인코더와 디코더의 임베딩 층을 각각 구현해보세요.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "- 참고 : https://wikidocs.net/86900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab3378c",
   "metadata": {},
   "source": [
    "### 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e36a830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "hidden_size = 512\n",
    "encoder_inputs = Input(shape=(None, ), name='encoder_input')\n",
    "enc_emb =  Embedding(eng_vocab_size, embedding_size,\n",
    "                    input_length=max_eng_seq_len)(encoder_inputs)\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)\n",
    "encoder_lstm = LSTM(hidden_size, dropout = 0.5, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048119f2",
   "metadata": {},
   "source": [
    "### 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af69daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, ), name='decoder_input')\n",
    "dec_emb =  Embedding(fra_vocab_size, embedding_size)(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "decoder_lstm = LSTM(hidden_size, dropout = 0.5, return_sequences = True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "952dc15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d6fa925",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d40f31fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 512)    2312192     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 512)    3719680     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 512)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 512)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 512), (None, 2099200     masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 512),  2099200     masking_1[0][0]                  \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7265)   3726945     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 13,957,217\n",
      "Trainable params: 13,957,217\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df0ced19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "938/938 [==============================] - 26s 19ms/step - loss: 1.4289 - val_loss: 1.2010\n",
      "Epoch 2/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 1.0920 - val_loss: 1.0533\n",
      "Epoch 3/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.9609 - val_loss: 0.9660\n",
      "Epoch 4/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.8731 - val_loss: 0.9081\n",
      "Epoch 5/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.8060 - val_loss: 0.8650\n",
      "Epoch 6/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.7514 - val_loss: 0.8333\n",
      "Epoch 7/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.7117 - val_loss: 0.8195\n",
      "Epoch 8/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.6830 - val_loss: 0.8127\n",
      "Epoch 9/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.6562 - val_loss: 0.7994\n",
      "Epoch 10/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.6285 - val_loss: 0.7961\n",
      "Epoch 11/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.6166 - val_loss: 0.8011\n",
      "Epoch 12/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.6120 - val_loss: 0.8073\n",
      "Epoch 13/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5989 - val_loss: 0.7982\n",
      "Epoch 14/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5871 - val_loss: 0.7995\n",
      "Epoch 15/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5742 - val_loss: 0.8076\n",
      "Epoch 16/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5729 - val_loss: 0.8082\n",
      "Epoch 17/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5690 - val_loss: 0.8062\n",
      "Epoch 18/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5535 - val_loss: 0.7907\n",
      "Epoch 19/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5461 - val_loss: 0.8009\n",
      "Epoch 20/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5467 - val_loss: 0.8028\n",
      "Epoch 21/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5433 - val_loss: 0.8029\n",
      "Epoch 22/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5383 - val_loss: 0.8040\n",
      "Epoch 23/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5327 - val_loss: 0.8004\n",
      "Epoch 24/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5252 - val_loss: 0.8001\n",
      "Epoch 25/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5205 - val_loss: 0.7992\n",
      "Epoch 26/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5140 - val_loss: 0.7947\n",
      "Epoch 27/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5076 - val_loss: 0.7930\n",
      "Epoch 28/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5019 - val_loss: 0.7965\n",
      "Epoch 29/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4981 - val_loss: 0.7916\n",
      "Epoch 30/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4920 - val_loss: 0.7880\n",
      "Epoch 31/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4881 - val_loss: 0.7869\n",
      "Epoch 32/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.4839 - val_loss: 0.7882\n",
      "Epoch 33/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4807 - val_loss: 0.7859\n",
      "Epoch 34/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4773 - val_loss: 0.7858\n",
      "Epoch 35/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4753 - val_loss: 0.7861\n",
      "Epoch 36/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4732 - val_loss: 0.7857\n",
      "Epoch 37/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4719 - val_loss: 0.7833\n",
      "Epoch 38/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4706 - val_loss: 0.7849\n",
      "Epoch 39/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.4676 - val_loss: 0.7838\n",
      "Epoch 40/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4669 - val_loss: 0.7847\n",
      "Epoch 41/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4653 - val_loss: 0.7829\n",
      "Epoch 42/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4636 - val_loss: 0.7867\n",
      "Epoch 43/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.4631 - val_loss: 0.7831\n",
      "Epoch 44/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4617 - val_loss: 0.7850\n",
      "Epoch 45/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4595 - val_loss: 0.7846\n",
      "Epoch 46/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4585 - val_loss: 0.7817\n",
      "Epoch 47/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4564 - val_loss: 0.7842\n",
      "Epoch 48/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4554 - val_loss: 0.7842\n",
      "Epoch 49/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4544 - val_loss: 0.7838\n",
      "Epoch 50/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4530 - val_loss: 0.7825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdb7dd88130>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], \n",
    "          y=decoder_target_train, \n",
    "          validation_data = ([encoder_input_test, decoder_input_test], \n",
    "                             decoder_target_test),\n",
    "          batch_size=32, \n",
    "          epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254afa9d",
   "metadata": {},
   "source": [
    "## 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2329d8d",
   "metadata": {},
   "source": [
    "### 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4efc8fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 512)         2312192   \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 512), (None, 512) 2099200   \n",
      "=================================================================\n",
      "Total params: 4,411,392\n",
      "Trainable params: 4,411,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f71a0",
   "metadata": {},
   "source": [
    "### 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7eb27cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(embedding_size,))\n",
    "decoder_state_input_c = Input(shape=(embedding_size,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = Embedding(fra_vocab_size, embedding_size)(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e87ae737",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a4f5cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 512)    3719680     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 512),  2099200     embedding_2[0][0]                \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7265)   3726945     lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 9,545,825\n",
      "Trainable params: 9,545,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d875c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <start>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = fra2idx['<start>']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<end>' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1221b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2src(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + idx2eng[i]+' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f57100b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=fra2idx['<start>']) and i!=fra2idx['<end>']):\n",
    "            temp = temp + idx2fra[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84db18f",
   "metadata": {},
   "source": [
    "## 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "879a29de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: i m busy , too . \n",
      "정답 문장: je suis galement affair . \n",
      "번역기가 번역한 문장:  je suis galemen\n",
      "-----------------------------------\n",
      "입력 문장: i need some advice . \n",
      "정답 문장: j ai besoin de quelques conseils . \n",
      "번역기가 번역한 문장:  j me besoin des d\n",
      "-----------------------------------\n",
      "입력 문장: hey , what s up ? \n",
      "정답 문장: h ! quoi de neuf ? \n",
      "번역기가 번역한 문장:  h , , , ? ? ? ? \n",
      "-----------------------------------\n",
      "입력 문장: how are you now ? \n",
      "정답 문장: comment allez vous maintenant ? \n",
      "번역기가 번역한 문장:  comment vas vou\n",
      "-----------------------------------\n",
      "입력 문장: tom died from tb . \n",
      "정답 문장: tom est mort de la tuberculose . \n",
      "번역기가 번역한 문장:  tom tom mort mor\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [1,201,501,1004,2015]:\n",
    "    input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', seq2src(encoder_input_test[seq_index]))\n",
    "    print('정답 문장:', seq2tar(decoder_input_test[seq_index]))\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c2b28",
   "metadata": {},
   "source": [
    "## 프로젝트 결과 및 회고\n",
    "- 이번 프로젝트는 변역기를 만드는 프로젝트 입니다. 데이터를 전처리하고 토큰화 및 패딩을 한 뒤 인코더, 디코더의 형태로 임베딩 층을 사용하여 모델을 구현하였습니다.\n",
    "- 번역기를 통과한 결과를 확인해본 결과 완벽하게 정답과 일치하지는 않아도 정답 문장의 핵심적인 단어를 언급하는 것을 확인할 수 있었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8403936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
